{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ced9b60",
   "metadata": {},
   "source": [
    "# Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26709e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import pickle\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28395b90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet==2023.10.1\n",
      "  Downloading fastparquet-2023.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m20.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas>=1.5.0 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from fastparquet==2023.10.1) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from fastparquet==2023.10.1) (1.24.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from fastparquet==2023.10.1) (2.8.2)\n",
      "Requirement already satisfied: fsspec in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from fastparquet==2023.10.1) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from fastparquet==2023.10.1) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet==2023.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet==2023.10.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet==2023.10.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/deeplearningtitan/anaconda3/envs/dsu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet==2023.10.1) (1.16.0)\n",
      "Installing collected packages: fastparquet\n",
      "  Attempting uninstall: fastparquet\n",
      "    Found existing installation: fastparquet 2024.2.0\n",
      "    Uninstalling fastparquet-2024.2.0:\n",
      "      Successfully uninstalled fastparquet-2024.2.0\n",
      "Successfully installed fastparquet-2023.10.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastparquet==2023.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ea3f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babadd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset of bins\n",
    "dataset = os.listdir(\"../../datasets/Canada_bins_df_final/\")\n",
    "\n",
    "# SELECT columns, Phi60_Sig1 will be added automatically in function create_windows\n",
    "_features_cols = [\"PC\"]\n",
    "\n",
    "# SELECT bins or use dataset for all bins \n",
    "_bins = dataset\n",
    "# SET window_size\n",
    "window_size = 45\n",
    "\n",
    "# SET how many minutes ahead do we want to predict\n",
    "minutes_ahead = 15\n",
    "\n",
    "# SET the folder name\n",
    "folder_name= \"PC\"\n",
    "\n",
    "# please don't change this parameter yet so we can compare on the same dataset\n",
    "test_year = 2019\n",
    "validation_year = 2015\n",
    "\n",
    "path_to_omni = \"../../datasets/omni4col_INTER_NORMALIZED_hour_2013-2021.parquet.gzip\"\n",
    "\n",
    "f_name = f\"{folder_name}/shift-{minutes_ahead}-windows-{window_size}/\"\n",
    "os.makedirs(f\"data/{f_name}/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740e452b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../datasets/Canada_bins_df_final/12844.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2168ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SVID', 'Azimuth', 'Elevation', 'AvgSig1_C/N0', 'S4_Sig1',\n",
       "       'Cor_S4_Sig1', 'Phi60_Sig1', 'AvgCCD_Sig1', 'TEC_TOW', 'Sig1_lock_time',\n",
       "       'Lock_time_2nd_freq', 'Avg_C/N0_2nd_freq', 'SI_ind_Sig1', 'p_Sig1',\n",
       "       'Latitude_IPP', 'Longitude_IPP', 'Latitude_bin', 'Longitude_bin',\n",
       "       'Bin_index', 'ne', 'Ti', 'nO2+', 'NmE', 'N2[cm^-3]', 'O2[cm^-3]',\n",
       "       'Exospheric_temp[K]', 'B', 'BzGSE', 'FlowPressure', 'E', 'Xbsn', 'AE',\n",
       "       'SymH', 'AsyD', 'AsyH', 'PC', 'Kp_index', 'Dst_index', 'ap_index',\n",
       "       'f10.7_index', 'Sun_altitude', 'Sun-Earth_distance', 'Variation_coeff',\n",
       "       'Binary_scinti_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d3e02",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ac8869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refactored function for aspis\n",
    "def create_windows(df, window_size, minutes_ahead, features_cols, separate_windows=True, path_to_omni_hour=None):\n",
    "    \"\"\"\n",
    "    Creates windows of the specified size and makes a time-shift.\n",
    "\n",
    "        Parameters:\n",
    "                df (Pandas DF): Data frame with features and label\n",
    "                window_size (int): Features window size (in minutes)\n",
    "                minutes_ahead (int): By how many minutes to make a prediction\n",
    "                features_cols (list): List of features to be used for prediction\n",
    "                separate_windows (bool): If true, returns windows with and without scintillation separately (training),\n",
    "                                         if false, returns all windows at once (testing)\n",
    "                path_to_omni_hour (path): path to Pandas DF with data from OMNIWeb\n",
    "\n",
    "        Returns:\n",
    "            separate_windows is True:\n",
    "                X_min_scinti (numpy array): Minute resolution feature windows with scintillation\n",
    "                X_hour_scinti (numpy array): Hour resolution feature values with scintillation\n",
    "                y_scinti (numpy array): Label (regression values) with scintillation\n",
    "                X_min_no_scinti (numpy array): Minute resolution feature windows without scintillation\n",
    "                X_hour_no_scinti (numpy array): Hour resolution feature values without scintillation\n",
    "                y_no_scinti (numpy array): Label (regression values) without scintillation\n",
    "\n",
    "            separate_windows is False:\n",
    "                X_min (numpy array): Minute resolution feature windows\n",
    "                X_hour (numpy array): Hour resolution feature values\n",
    "                y (numpy array): Label (regression values)\n",
    "                scinti_label (numpy array): Label (binary values)\n",
    "                index_to_save (numpy array): Time index for visualisation purposes\n",
    "    \"\"\"\n",
    "\n",
    "    # return empty arrays, if data frame is too small\n",
    "    if len(df) < (window_size + minutes_ahead):\n",
    "        if separate_windows:\n",
    "            return np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "        else:\n",
    "            np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    if \"Phi60_Sig1\" in features_cols:\n",
    "        features_cols.remove(\"Phi60_Sig1\")\n",
    "    if \"Binary_scinti_label\" in features_cols:\n",
    "        features_cols.remove(\"Binary_scinti_label\")\n",
    "    features_cols.append(\"Phi60_Sig1\")\n",
    "    features_cols.append(\"Binary_scinti_label\")\n",
    "\n",
    "    df.loc[df['Phi60_Sig1'] > 1, 'Phi60_Sig1'] = 1\n",
    "\n",
    "    hourly_features = [\"Kp_index\", \"Dst_index\", \"ap_index\", \"f10.7_index\"]\n",
    "    hourly_features_used = []\n",
    "    minute_features_used = []\n",
    "    for col in features_cols:\n",
    "        if col in hourly_features:\n",
    "            hourly_features_used.append(col)\n",
    "        else:\n",
    "            minute_features_used.append(col)\n",
    "\n",
    "    # fills missing indexes\n",
    "    new_date_range = pd.date_range(start=df.index[0], end=df.index[-1], freq=\"min\")\n",
    "    df = df.reindex(new_date_range, fill_value=np.nan)\n",
    "\n",
    "    # calculates the number of windows and prepares an empty array\n",
    "    n_of_windows = len(df) - window_size + 1\n",
    "    X_min = np.ones((n_of_windows, len(minute_features_used), window_size))\n",
    "    if len(hourly_features_used) > 0:\n",
    "        X_hour = np.ones((n_of_windows, len(hourly_features_used), 1))\n",
    "\n",
    "        if path_to_omni_hour is not None:\n",
    "            omni_hour = pd.read_parquet(path_to_omni_hour)\n",
    "            omni_hour = omni_hour.loc[str(df.index[0]):str(df.index[-1])]\n",
    "            df[hourly_features_used] = omni_hour[hourly_features_used]\n",
    "        else:\n",
    "            raise ValueError('The path to the OMNI hourly dataset is not specified!')\n",
    "    else:\n",
    "        X_hour = np.array([])\n",
    "\n",
    "    # fills the array of minute features with values\n",
    "    for i in range(-1, (window_size + 1) * -1, -1):\n",
    "        if i == -1:\n",
    "            X_min[:, :, i] = X_min[:, :, i] * np.array(df[minute_features_used])[window_size + i:]\n",
    "        else:\n",
    "            X_min[:, :, i] = X_min[:, :, i] * np.array(df[minute_features_used])[window_size + i:i + 1]\n",
    "\n",
    "    # fills the array of hour features with values\n",
    "    if len(hourly_features_used) > 0:\n",
    "        X_hour[:, :, -1] = X_hour[:, :, -1] * np.array(df[hourly_features_used])[window_size - 1:]\n",
    "\n",
    "    # fills the array of labels with values\n",
    "    y = np.array(df['Phi60_Sig1'])[window_size + minutes_ahead - 1:]\n",
    "    y = np.append(y, np.ones(minutes_ahead) * np.nan)\n",
    "\n",
    "    # fills the array of scintillation labels with values\n",
    "    scinti_label = np.array(df['Binary_scinti_label'])[window_size + minutes_ahead - 1:]\n",
    "    scinti_label = np.append(scinti_label, np.ones(minutes_ahead) * np.nan)\n",
    "    index_to_save = np.array(df.index)[window_size + minutes_ahead - 1:]\n",
    "    for i in range(minutes_ahead):\n",
    "        index_to_save = np.append(index_to_save, pd.NaT)\n",
    "\n",
    "    # delete windows with NaN values\n",
    "    X_min_index_nan = np.argwhere(np.isnan(X_min))\n",
    "    X_hour_index_nan = np.argwhere(np.isnan(X_hour))\n",
    "    y_index_nan = np.argwhere(np.isnan(y))\n",
    "    rows_with_nan = np.unique(np.append(np.unique(X_min_index_nan[:, 0]), np.append(np.unique(X_hour_index_nan[:, 0]), y_index_nan[:, 0])))\n",
    "\n",
    "    X_min = np.delete(X_min, rows_with_nan, axis=0)\n",
    "    if len(hourly_features_used) > 0:\n",
    "        X_hour = np.delete(X_hour, rows_with_nan, axis=0)\n",
    "    y = np.delete(y, rows_with_nan, axis=0)\n",
    "    scinti_label = np.delete(scinti_label, rows_with_nan, axis=0)\n",
    "    index_to_save = np.delete(index_to_save, rows_with_nan, axis=0)\n",
    "    \n",
    "    # column 'Phi60_Sig1' must be dropped because somehow it propagates out of function and normalization doesn't work properly\n",
    "    features_cols.pop()\n",
    "    features_cols.pop()\n",
    "    if separate_windows:\n",
    "        index_scinti = np.argwhere(scinti_label == 1)\n",
    "        X_min_scinti = X_min[index_scinti]\n",
    "        y_scinti = y[index_scinti]\n",
    "        \n",
    "        index_to_save_scinti = index_to_save[index_scinti]\n",
    "        \n",
    "        X_min_no_scinti = np.delete(X_min, index_scinti, axis=0)\n",
    "        y_no_scinti = np.delete(y, index_scinti, axis=0)\n",
    "\n",
    "        index_to_save_no_scinti = np.delete(index_to_save, index_scinti, axis=0)\n",
    "        \n",
    "        if len(hourly_features_used) > 0:\n",
    "            X_hour_scinti = X_hour[index_scinti]\n",
    "            X_hour_scinti = X_hour_scinti[:, 0, :]\n",
    "            X_hour_no_scinti = np.delete(X_hour, index_scinti, axis=0)\n",
    "        else:\n",
    "            X_hour_scinti = np.array([])\n",
    "            X_hour_no_scinti = np.array([])\n",
    "\n",
    "        if len(y_scinti) == 0:\n",
    "            return np.array([]), np.array([]), np.array([]), X_min_no_scinti, X_hour_no_scinti, y_no_scinti, np.array([]), index_to_save_no_scinti\n",
    "        else:\n",
    "            return X_min_scinti[:, 0, :], X_hour_scinti, y_scinti[:, 0], X_min_no_scinti, X_hour_no_scinti, y_no_scinti, index_to_save_scinti[:, 0], index_to_save_no_scinti\n",
    "\n",
    "    return X_min, X_hour, y, scinti_label, index_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90197f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_classes(X_train_min_windows_scinti, X_train_hour_windows_scinti, y_train_windows_scinti,\n",
    "                    X_train_min_windows_no_scinti, X_train_hour_windows_no_scinti, y_train_windows_no_scinti,\n",
    "                    index_to_save_scinti, index_to_save_no_scinti):\n",
    "    indexes = random.sample(range(len(X_train_min_windows_no_scinti)),\n",
    "                            min(len(X_train_min_windows_no_scinti), len(X_train_min_windows_scinti)))\n",
    "\n",
    "    # Ensure both arrays have the same number of dimensions\n",
    "    X_train_min_windows_no_scinti = np.array([X_train_min_windows_no_scinti[i] for i in indexes])\n",
    "    X_train_min_windows_scinti = np.array(X_train_min_windows_scinti)\n",
    "\n",
    "    if X_train_min_windows_no_scinti.size > 0 and X_train_min_windows_scinti.size > 0:\n",
    "        X_train_min_windows = np.concatenate((X_train_min_windows_no_scinti, X_train_min_windows_scinti), axis=0)\n",
    "    else:\n",
    "        X_train_min_windows = np.array([])\n",
    "\n",
    "    if len(X_train_hour_windows_scinti) != 0:\n",
    "        X_train_hour_windows_no_scinti = np.array([X_train_hour_windows_no_scinti[i] for i in indexes])\n",
    "        X_train_hour_windows_scinti = np.array(X_train_hour_windows_scinti)\n",
    "        X_train_hour_windows = np.concatenate((X_train_hour_windows_no_scinti, X_train_hour_windows_scinti), axis=0)\n",
    "    else:\n",
    "        X_train_hour_windows = np.array([])\n",
    "\n",
    "    y_train_windows = np.concatenate((y_train_windows_no_scinti[indexes], y_train_windows_scinti), axis=0)\n",
    "    index_to_save_train = np.concatenate((index_to_save_no_scinti[indexes], index_to_save_scinti), axis=0)\n",
    "    print(\"total train windows: \", len(y_train_windows))\n",
    "    print(\"total train time indexes to save: \", len(index_to_save_train))\n",
    "    return X_train_min_windows, X_train_hour_windows, y_train_windows, index_to_save_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344e1e1",
   "metadata": {},
   "source": [
    "# Create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee1f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating normalization values, bin:  13388.parquet.gzip\n",
      "Calculating normalization values, bin:  13379.parquet.gzip\n",
      "Som v &create_windows&\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "D1\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "D2\n",
      "E\n",
      "E1\n",
      "E2\n"
     ]
    }
   ],
   "source": [
    "# choose selection of bins or keep all bins\n",
    "#_bins=[\"13388.parquet.gzip\"]\n",
    "\n",
    "X_train_min_final = []\n",
    "X_train_hour_final = []\n",
    "y_train_final = []\n",
    "index_to_save_train_final = []\n",
    "bins_to_save_train_final = []\n",
    "\n",
    "X_test_min_final = []\n",
    "X_test_hour_final = []\n",
    "y_test_final = []\n",
    "index_to_save_test_final = []\n",
    "bins_to_save_test_final = []\n",
    "\n",
    "X_val_min_final = []\n",
    "X_val_hour_final = []\n",
    "y_val_final = []\n",
    "index_to_save_val_final = []\n",
    "bins_to_save_val_final = []\n",
    "\n",
    "# min-max normalization values\n",
    "max_values = [-np.inf] * len(_features_cols)\n",
    "min_values = [np.inf] * len(_features_cols)\n",
    "\n",
    "for bins in _bins:\n",
    "    print(\"Calculating normalization values, bin: \", bins)\n",
    "    df = pd.read_parquet(\"../../datasets/Canada_bins_df_final/\"+bins)\n",
    "    train = df[df.index.year != test_year]\n",
    "    train = train[train.index.year != validation_year]\n",
    "    \n",
    "    for i in range(len(_features_cols)):\n",
    "        \n",
    "        max_col_value = max(train[_features_cols[i]])\n",
    "        if max_col_value > max_values[i]:\n",
    "            max_values[i] = max_col_value\n",
    "            \n",
    "        min_col_value = min(train[_features_cols[i]])\n",
    "        if min_col_value < min_values[i]:\n",
    "            min_values[i] = min_col_value\n",
    "\n",
    "for bins in _bins:\n",
    "    # read data\n",
    "    df = pd.read_parquet(\"../../datasets/Canada_bins_df_final/\"+bins)\n",
    "    \n",
    "    # normalize\n",
    "    fn = lambda value, x_max, x_min: (value - x_min) / (x_max - x_min)\n",
    "    \n",
    "    for i in range(len(_features_cols)):\n",
    "        df[_features_cols[i]] = fn(df[_features_cols[i]], max_values[i], min_values[i])\n",
    "    \n",
    "    # define train, test and valid subset\n",
    "    train = df[df.index.year != test_year]\n",
    "    train = train[train.index.year != validation_year]\n",
    "    test = df[df.index.year == test_year]\n",
    "    valid = df[df.index.year == validation_year]\n",
    "    \n",
    "    X_min_scinti, X_hour_scinti, y_scinti, X_min_no_scinti, X_hour_no_scinti, y_no_scinti, time_scinti, time_no_scinti = create_windows(train, window_size, minutes_ahead, _features_cols, separate_windows=True, path_to_omni_hour=path_to_omni)\n",
    "    \n",
    "    if len(X_min_scinti) == 0 or len(y_scinti) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(\"TRAIN set\")\n",
    "    X_train_min_windows, X_train_hour_windows, y_train_windows, index_to_save_train = balance_classes(X_min_scinti, X_hour_scinti, y_scinti, X_min_no_scinti, X_hour_no_scinti, y_no_scinti, time_scinti, time_no_scinti)\n",
    "    \n",
    "    X_test_min_windows, X_test_hour_windows, y_test_windows, scinti_label_test, index_to_save_test = create_windows(test, window_size, minutes_ahead, _features_cols, separate_windows=False, path_to_omni_hour=path_to_omni)\n",
    "    X_val_min_windows, X_val_hour_windows, y_val_windows, scinti_label_val, index_to_save_val = create_windows(valid, window_size, minutes_ahead, _features_cols, separate_windows=False, path_to_omni_hour=path_to_omni)\n",
    "    \n",
    "    print(\"Bin: \", bins)\n",
    "    print(\"TEST set\")\n",
    "    \n",
    "    print(\"total train\")\n",
    "    X_train_min_final.extend(X_train_min_windows)\n",
    "    X_train_hour_final.extend(X_train_hour_windows)\n",
    "    print(len(X_train_min_final))\n",
    "    y_train_final.extend(y_train_windows)\n",
    "    index_to_save_train_final.extend(index_to_save_train)\n",
    "    bins_to_save_train_final.extend([bins]*len(index_to_save_train))\n",
    "    \n",
    "    print(\"total test\")\n",
    "    X_test_min_final.extend(X_test_min_windows)\n",
    "    X_test_hour_final.extend(X_test_hour_windows)\n",
    "    print(len(X_test_min_final))\n",
    "    y_test_final.extend(y_test_windows)\n",
    "    index_to_save_test_final.extend(index_to_save_test)\n",
    "    bins_to_save_test_final.extend([bins]*len(index_to_save_test))\n",
    "    \n",
    "    print(\"total validation\")\n",
    "    X_val_min_final.extend(X_val_min_windows)\n",
    "    X_val_hour_final.extend(X_val_hour_windows)\n",
    "    print(len(X_val_min_final))\n",
    "    y_val_final.extend(y_val_windows)\n",
    "    index_to_save_val_final.extend(index_to_save_val)\n",
    "    bins_to_save_val_final.extend([bins]*len(index_to_save_val))\n",
    "    \n",
    "    print(\"___________next bin________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49b700",
   "metadata": {},
   "source": [
    "# Create dict and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848e4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ame = _features_cols + [\"Phi60_Sig1\"]\n",
    "hour_features = [\"Kp_index\", \"Dst_index\", \"ap_index\", \"f10.7_index\"]\n",
    "hour_features_used = []\n",
    "minute_features_used = []\n",
    "\n",
    "for col in name:\n",
    "    if col in hour_features:\n",
    "        hour_features_used.append(col)\n",
    "    else:\n",
    "        minute_features_used.append(col)\n",
    "\n",
    "\n",
    "train_min = {key:[] for key in minute_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(train_min):\n",
    "    for col in train_min:\n",
    "        for i in X_train_min_final:\n",
    "            train_min[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in train_min:\n",
    "    train_min[col]=np.vstack(train_min[col])\n",
    "\n",
    "\n",
    "train_hour = {key:[] for key in hour_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(train_hour):\n",
    "    for col in train_hour:\n",
    "        for i in X_train_hour_final:\n",
    "            train_hour[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in train_hour:\n",
    "    train_hour[col]=np.vstack(train_hour[col])\n",
    "\n",
    "\n",
    "test_min = {key:[] for key in minute_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(test_min):\n",
    "    for col in test_min:\n",
    "        for i in X_test_min_final:\n",
    "            test_min[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in test_min:\n",
    "    test_min[col]=np.vstack(test_min[col])\n",
    "\n",
    "\n",
    "test_hour = {key:[] for key in hour_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(test_hour):\n",
    "    for col in test_hour:\n",
    "        for i in X_test_hour_final:\n",
    "            test_hour[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in test_hour:\n",
    "    test_hour[col]=np.vstack(test_hour[col])\n",
    "\n",
    "\n",
    "\n",
    "val_min = {key:[] for key in minute_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(val_min):\n",
    "    for col in val_min:\n",
    "        for i in X_val_min_final:\n",
    "            val_min[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in val_min:\n",
    "    val_min[col]=np.vstack(val_min[col])\n",
    "\n",
    "\n",
    "val_hour = {key:[] for key in hour_features_used}\n",
    "n=0\n",
    "\n",
    "while n<len(val_hour):\n",
    "    for col in val_hour:\n",
    "        for i in X_val_hour_final:\n",
    "            val_hour[col].append(i[n])\n",
    "        n = n+1\n",
    "\n",
    "for col in val_hour:\n",
    "    val_hour[col]=np.vstack(val_hour[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_val_final=[datetime.datetime.utcfromtimestamp(int(timestamp)/1000000000).strftime('%Y-%m-%d %H:%M:%S') for timestamp in index_to_save_val_final]\n",
    "index_test_final=[datetime.datetime.utcfromtimestamp(int(timestamp)/1000000000).strftime('%Y-%m-%d %H:%M:%S') for timestamp in index_to_save_test_final]\n",
    "index_train_final=[datetime.datetime.utcfromtimestamp(int(timestamp)/1000000000).strftime('%Y-%m-%d %H:%M:%S') for timestamp in index_to_save_train_final]\n",
    "\n",
    "y_test = np.vstack(y_test_final)\n",
    "y_train = np.vstack(y_train_final)\n",
    "y_val = np.vstack(y_val_final)\n",
    "\n",
    "index_val = np.vstack(index_val_final)\n",
    "index_test = np.vstack(index_test_final)\n",
    "index_train = np.vstack(index_train_final)\n",
    "\n",
    "bin_val = np.vstack(bins_to_save_val_final)\n",
    "bin_test = np.vstack(bins_to_save_test_final)\n",
    "bin_train = np.vstack(bins_to_save_train_final)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4743a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/{f_name}/y_train.npy', 'wb') as f:\n",
    "    pickle.dump(y_train, f, protocol=4)\n",
    "with open(f'data/{f_name}/y_test.npy', 'wb') as f:\n",
    "    pickle.dump(y_test, f, protocol=4)\n",
    "with open(f'data/{f_name}/y_val.npy', 'wb') as f:\n",
    "    pickle.dump(y_val, f, protocol=4)\n",
    "\n",
    "with open(f'data/{f_name}/X_train_min.npy', 'wb') as f:\n",
    "    pickle.dump(train_min, f, protocol=4)\n",
    "with open(f'data/{f_name}/X_test_min.npy', 'wb') as f:\n",
    "    pickle.dump(test_min, f, protocol=4)\n",
    "with open(f'data/{f_name}/X_val_min.npy', 'wb') as f:\n",
    "    pickle.dump(val_min, f, protocol=4)\n",
    "\n",
    "with open(f'data/{f_name}/X_train_hour.npy', 'wb') as f:\n",
    "    pickle.dump(train_hour, f, protocol=4)\n",
    "with open(f'data/{f_name}/X_test_hour.npy', 'wb') as f:\n",
    "    pickle.dump(test_hour, f, protocol=4)\n",
    "with open(f'data/{f_name}/X_val_hour.npy', 'wb') as f:\n",
    "    pickle.dump(val_hour, f, protocol=4)\n",
    "\n",
    "with open(f'data/{f_name}/v3/test_index_timestamp.npy', 'wb') as f:\n",
    "    pickle.dump(index_test, f, protocol=4)\n",
    "with open(f'data/{f_name}/v3/test_index_bin.npy', 'wb') as f:\n",
    "    pickle.dump(bin_test, f, protocol=4)\n",
    "\n",
    "with open(f'data/{f_name}/v3/train_index_timestamp.npy', 'wb') as f:\n",
    "    pickle.dump(index_train, f, protocol=4)\n",
    "with open(f'data/{f_name}/v3/train_index_bin.npy', 'wb') as f:\n",
    "    pickle.dump(bin_train, f, protocol=4)\n",
    "\n",
    "with open(f'data/{f_name}/v3/val_index_timestamp.npy', 'wb') as f:\n",
    "    pickle.dump(index_val, f, protocol=4)\n",
    "with open(f'data/{f_name}/v3/val_index_bin.npy', 'wb') as f:\n",
    "    pickle.dump(bin_val, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4c23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
